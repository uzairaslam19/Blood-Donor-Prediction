{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=green>Blood Donor Prediction(Warmup Blood Donation prediction challenge) </font> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question:: Can you predict whether a donor will return to donate blood given their donation history?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As per the instructions, the task is to predict whether a donor will give blood the next time the Blood Donation van Comes to the campus. First things first i need to load the data and explore it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=green>Importing Libraries and Data Loading</font> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing Libraries for Data exploration and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from IPython.core.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TransTrain= pd.read_csv('training_data.csv')\n",
    "TransTest=pd.read_csv('test_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The task was about a Blood donation mobile Van in Taiwan that visits a university campus each month. The task was to determine whether one can predict based on past data if a customer or donor would donate blood on march of 2007. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=green>Flow Chart</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](img/2019.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=green>Data Exploration</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Lets take a look at what the training data looks like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(TransTrain.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The First column is the ID column, i would've Removed it if it was not required for submission along with Predicted Values. Therefore, i will change it's name To Client Id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TransTrain.rename(columns={'Unnamed: 0':'Client_ID'}, inplace=True)\n",
    "TransTest.rename(columns={'Unnamed: 0':'Client_ID'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(TransTrain.shape)\n",
    "print(TransTest.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the Training data has 576 observations in total and testing set is smaller in size with 200 observations. lets see if there are any missing values in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TransTrain.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TransTest.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apparently there are no missing values, lets explore further and see what are the data types for each Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(TransTrain.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=green>Descriptive Statistics </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data seems to be clean and does not contain any missing values, this is good news as i dont need to worry about data cleaning. However, i need to check if my data has any outliers that need to be treated "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TransTrain.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok so the descriptive statics of my training data shows a much clearer picture of the data. I can see that many of my variables have outliers for example in the case of \" Number of donations\" the min and max values are 1 and 50 respectively but the mean is quite low at 5.42. This means that the max value is that of an outlier. I will need to treat these. Similar cases for other variables as well as \"Months since Last Donation\" has min and max values 0 and 74 but mean is again quite low at 9.439. These outliers need to be handled before moving forward to modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I want to see how my variables are correlated first i will use Pairplot from seaborn library to see the distribution of each variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(TransTrain, diag_kind='kde',hue='Made Donation in March 2007')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above plot alot of things become clear, we can clearly see that first of all the target variable has only 1 and 0 values and there are no ambiguities in it. We can also see that two variables seem to be highly correlated with eachother as they seem to have highly linear relation. These are Number of Donations and Total Volume donated. This is not surprising as the total volume donated is directly proportional to how many times a customer donated blood. However, since both these are correlated this may cause a problem during model building. I will need to remove one of them but i need to be sure of this. For this i will create a heat map\n",
    "\n",
    "\n",
    "Furthermore, the pairplot also confirms the existance of outliers in multiple variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Are any of the variables highly correlated with eachother?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = TransTrain.corr()\n",
    "# Correlation Plot for the independent variables\n",
    "fig, ax = plt.subplots(figsize=(25,15))\n",
    "sns.heatmap(corr, annot=True,\n",
    "        xticklabels=corr.columns,\n",
    "        yticklabels=corr.columns, cmap=\"YlGnBu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above heat map clearly shows that the two variables are highly correlated with each other\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I still need to make sure that my Target Variable doesnt contain any ambigous values or whether there exists any sort of class imbalance in my Target Variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What is the Distribution of Target Variable?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TransTrain['Made Donation in March 2007'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This shows that my data is imbalanced, let's plot it to see a clear picture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(20,6))\n",
    "TransTrain['Made Donation in March 2007'].value_counts().plot(kind='bar', subplots=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "People who have donated in March 2007 is 4 times less than those who havent donated blood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=green>Feature Engineering and Feature Selection</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before i get rid of the correlated features, i will create some additional variables. Lets Create a Feature for who are Frequent Donors, we can extract this information from the number of donations feature. Since the mean of Number of Donations is 5. We will create a new categorical feature of whether a donor is frequenct donor or not if they have donated more then 5 times. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TransTrain['Frequenct Donor'] = (TransTrain['Number of Donations'] >= 5)\n",
    "TransTest['Frequenct Donor'] = (TransTest['Number of Donations'] >= 5)\n",
    "display(TransTrain.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will create another feature to see how long does a donor wait from their last donation to donate blood again. For this i can simply subtract Last month donations from First month donations and divide them by number of donations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TransTrain['Donation Frequency'] = ((TransTrain['Months since First Donation'] - TransTrain['Months since Last Donation'])\n",
    "                           /TransTrain['Number of Donations'])\n",
    "TransTest['Donation Frequency'] = ((TransTest['Months since First Donation'] - TransTest['Months since Last Donation'])\n",
    "                           /TransTest['Number of Donations'])\n",
    "display(TransTrain.head(15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (15, 5))\n",
    "\n",
    "sns.distplot(TransTrain[TransTrain['Made Donation in March 2007'].values == 0]['Donation Frequency'], color = 'Green')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Monthly Donation Frequency')\n",
    "\n",
    "sns.distplot(TransTrain[TransTrain['Made Donation in March 2007'].values == 1]['Donation Frequency'], color = 'yellow')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Monthly Donation Frequency ')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above plot we can see that Frequent donors are more likely to donate blood again. However there are many 0 values in the Donation frequency feature which means that these clients only donated blood once and never donated again."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have our additional Features, i will remove Total volume from the dataset now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TransTrain.drop(['Total Volume Donated (c.c.)'], axis=1, inplace=True)\n",
    "TransTest.drop(['Total Volume Donated (c.c.)'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets check the correlation matrix magain for our newly added features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = TransTrain.corr()\n",
    "# Correlation Plot for the independent variables\n",
    "fig, ax = plt.subplots(figsize=(25,15))\n",
    "sns.heatmap(corr, annot=True,\n",
    "        xticklabels=corr.columns,\n",
    "        yticklabels=corr.columns, cmap=\"BuPu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before i move on to anything else i need to Encode the categorical features to their respective numeric values for modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TransTrain['Frequenct Donor'] = TransTrain['Frequenct Donor'].astype('category')\n",
    "TransTest['Frequenct Donor'] = TransTest['Frequenct Donor'].astype('category')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TransTrain['Frequenct Donor'] = TransTrain['Frequenct Donor'].cat.codes\n",
    "TransTest['Frequenct Donor'] = TransTest['Frequenct Donor'].cat.codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(TransTrain.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=green>Splitting into Training and Validation Set</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install imblearn\n",
    "!pip install sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X= TransTrain.drop(['Made Donation in March 2007'],axis=1)\n",
    "Y= pd.DataFrame(TransTrain['Made Donation in March 2007'])\n",
    "x_test=TransTest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_val, y_train, y_val = train_test_split(X, Y,\n",
    "                                                  test_size = .1,\n",
    "                                                  random_state=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=green>Handling Class Imbalance Using Smote</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have split the data into training and validation set, lets handle the class imbalance using smote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm = SMOTE(random_state=12, ratio = 1.0)\n",
    "x_train_res, y_train_res = sm.fit_sample(x_train, y_train.values.ravel()) # using .values.ravel() because otherwise it gives an error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=green>Modelling</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import model_selection\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import log_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now i define a function that will print out a confusion matrix, Although the challenge requires the evalutaion matrics to be Log loss, i want to check the recall and accuracy of the model as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\"Defining A function for model evaulatin\"\"\"\n",
    "def model1(mod, model_name, x_train, y_train, x_test, y_test):\n",
    "    mod.fit(x_train_res, y_train_res)\n",
    "    print(model_name)\n",
    "    acc = cross_val_score(mod, x_train_res, y_train_res, scoring = \"neg_log_loss\", cv = 5)\n",
    "    predictions = cross_val_predict(mod, x_train_res, y_train_res, cv = 5)\n",
    "    print(\"Log Loss:\", log_loss(y_val, y_val_lr))\n",
    "    cm = confusion_matrix(predictions, y_train_res)\n",
    "    print(\"Confusion Matrix:  \\n\", cm)\n",
    "    print(\"                    Classification Report \\n\",classification_report(predictions, y_train_res))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color=green>Logistic Regression</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR=LogisticRegressionCV(max_iter=1000,scoring='neg_log_loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1(LR,\"Logistic Regression\",x_train_res,y_train_res,x_val,y_val)\n",
    "y_val_lr = LR.predict_proba(x_val)[:, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the recall for our Logit model is 68 percent for correctly predicting Blood donors, though we would like to see how it performs in terms of Log loss as well and we can see that Log loss is 0.541 Which seems quite less.. The lesser the better. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
